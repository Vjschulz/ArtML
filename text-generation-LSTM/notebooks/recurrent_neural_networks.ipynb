{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on:\n",
    "https://github.com/ml4a/ml4a-guides/blob/master/notebooks/recurrent_neural_networks.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks: Character RNNs with Keras\n",
    "\n",
    "Often we are not interested in isolated datapoints, but rather datapoints within a context of others. A datapoint may mean something different depending on what's come before it. This can typically be represented as some kind of _sequence_ of datapoints, perhaps the most common of which is a time series.\n",
    "\n",
    "One of the most ubiquitous sequences of data where context is especially important is natural language. We have quite a few words in English where the meaning of a word may be totally different depending on it's context. An innocuous example of this is \"bank\": \"I went fishing down by the river bank\" vs \"I deposited some money into the bank\".\n",
    "\n",
    "If we consider that each word is a datapoint, most non-recurrent methods will treat \"bank\" in the first sentence exactly the same as \"bank\" in the second sentence - they are indistinguishable. If you think about it, in isolation they are indistinguishable to us as well - it's the same word!\n",
    "\n",
    "We can only start to discern them when we consider the previous word (or words). So we might want our neural network to consider that \"bank\" in the first sentence is preceded by \"river\" and that in the second sentence \"money\" comes a few words before it. That's basically what RNNs do - they \"remember\" some of the previous context and that influences the output it produces. This \"memory\" (called the network's \"_hidden state_\") works by retaining some of the previous outputs and combining it with the current input; this recursing (feedback) of the network's output back into itself is where its name comes from.\n",
    "\n",
    "This recursing makes RNNs quite deep, and thus they can be difficult to train. The gradient gets smaller and smaller the deeper it is pushed backwards through the network until it \"vanishes\" (effectively becomes zero), so long-term dependencies are hard to learn. The typical practice is to only extend the RNN back a certain number of time steps so the network is still trainable.\n",
    "\n",
    "Certain units, such as the LSTM (long short-term memory) and GRU (gated recurrent unit), have been developed to mitigate some of this vanishing gradient effect.\n",
    "\n",
    "Let's walkthrough an example of a character RNN, which is a great approach for learning a character-level language model. A language model is essentially some function which returns a probability over possible words (or in this case, characters), based on what has been seen so far. This function can vary from region to region (e.g. if terms like \"pop\" are used more commonly than \"soda\") or from person to person. You could say that a (good) language model captures the style in which someone writes.\n",
    "\n",
    "Language models often must make the simplifying assumption that only what came immediately (one time step) before matters (this is called the \"Markov assumption\"), but with RNNs we do not need to make such an assumption.\n",
    "\n",
    "We'll use Keras which makes building neural networks extremely easy (this example is an annotated version of Keras's [LSTM text generation example](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py)).\n",
    "\n",
    "First we'll do some simple preparation - import the classes we need and load up the text we want to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#if using Theano with GPU\n",
    "#os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/sotu/Barack-Obama_2009.txt',\n",
       " '../data/sotu/George-W.-Bush_2006.txt',\n",
       " '../data/sotu/Dwight-D.-Eisenhower_1959.txt',\n",
       " '../data/sotu/Barack-Obama_2013.txt',\n",
       " '../data/sotu/Ronald-Reagan_1983.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load up our text\n",
    "text_files = glob('../data/sotu/*.txt')\n",
    "text_files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Madam Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States--she's around here somewhere: I have come here tonight not only to address the distinguished men and women in this great Chamber, but to speak frankly and directly to the men and women who sent us here. \\nI know that for many Americans watching right now, the state of our economy is a concern that rises above all others, and rightly so. If you haven't been personally affected by this recession, you probabl\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us create a long string variable text\n",
    "text = '\\n'.join([open(f, 'r').read() for f in text_files])\n",
    "text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n",
      "2942837\n"
     ]
    }
   ],
   "source": [
    "print(type(text))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '$', '(', ',', '0', '4', '8', 'D', 'H', 'L', 'P', 'T', 'X', '`', 'd', 'h', 'l', 'p', 't', 'x', \"'\", '+', '/', '3', '7', ';', '\\xbc', '?', 'C', 'G', 'K', 'O', 'S', 'W', '[', '_', 'c', 'g', 'k', 'o', 's', 'w', '\\n', '\\x95', '\"', '&', '*', '.', '2', '6', ':', '\\xbd', 'B', 'F', 'J', 'N', 'R', 'V', 'Z', 'b', 'f', 'j', 'n', 'r', 'v', 'z', '!', '\\xa2', '%', ')', '-', '1', '5', '9', 'A', '\\xc2', 'E', 'I', 'M', 'Q', 'U', 'Y', ']', 'a', 'e', 'i', 'm', 'q', 'u', 'y']\n"
     ]
    }
   ],
   "source": [
    "# extract all (unique) characters\n",
    "# these are our \"categories\" or \"labels\". We want to predict the next character from the past few (e.g 20) characters\n",
    "chars = list(set(text))\n",
    "print(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a fixed vector size\n",
    "# so we look at specific windows of characters\n",
    "max_len = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define our RNN. Keras makes this trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(max_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20, 512)           1234944   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 90)                46170     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 90)                0         \n",
      "=================================================================\n",
      "Total params: 3,380,314\n",
      "Trainable params: 3,380,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're framing our task as a classification task. Given a sequence of characters, we want to predict the next character. We equate each character with some label or category (e.g. \"a\" is 0, \"b\" is 1, etc).\n",
    "\n",
    "We use the _softmax_ activation function on our output layer - this function is used for categorical output. It turns the output into a probability distribution over the categories (i.e. it makes the values the network outputs sum to 1). So the network will essentially tell us how strongly it feels about each character being the next one.\n",
    "\n",
    "The categorical cross-entropy loss the standard loss function for multilabel classification, which basically penalizes the network more the further off it is from the correct label.\n",
    "\n",
    "We use dropout here to prevent overfitting - we don't want the network to just return things already in the text, we want it to have some wiggle room and create novelty! Dropout is a technique where, in training, some percent (here, 20%) of random neurons of the associated layer are \"turned off\" for that epoch. This prevents overfitting by preventing the network from relying on particular neurons.\n",
    "\n",
    "That's it for the network architecture!\n",
    "\n",
    "To train, we have to do some additional preparation. We need to chop up the text into character sequences of the length we specified (`max_len`) - these are our training inputs. We match them with the character that immediately follows each sequence. These are our expected training outputs.\n",
    "\n",
    "For example, say we have the following text (this quote is from Zhuang Zi). With `max_len=20`, we could manually create the first couple training examples like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fish trap exists\n",
      " \n"
     ]
    }
   ],
   "source": [
    "example_text = \"The fish trap exists because of the fish. Once you have gotten the fish you can forget the trap. The rabbit snare exists because of the rabbit. Once you have gotten the rabbit, you can forget the snare. Words exist because of meaning. Once you have gotten the meaning, you can forget the words. Where can I find a man who has forgotten words so that I may have a word with him?\"\n",
    "\n",
    "# step size here is 3, but we can vary that\n",
    "input_1 = example_text[0:20]\n",
    "true_output_1 = example_text[20]\n",
    "# >>> 'The fish trap exists'\n",
    "# >>> ' '\n",
    "print(input_1)\n",
    "print(true_output_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fish trap exists be\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "input_2 = example_text[3:23]\n",
    "true_output_2 = example_text[23]\n",
    "# >>> 'fish trap exists be'\n",
    "# >>> 'c'\n",
    "print(input_2)\n",
    "print(true_output_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh trap exists becau\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "input_3 = example_text[6:26]\n",
    "true_output_3 = example_text[26]\n",
    "# >>> 'sh trap exists becau'\n",
    "# >>> 's'\n",
    "\n",
    "# etc\n",
    "print(input_3)\n",
    "print(true_output_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generalize this like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 3\n",
    "inputs = []\n",
    "outputs = []\n",
    "for i in range(0, len(text) - max_len, step):\n",
    "    inputs.append(text[i:i+max_len])\n",
    "    outputs.append(text[i+max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980939, 980939)\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs),len(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"ington, we've all se\", 'e')\n"
     ]
    }
   ],
   "source": [
    "# Let us see a specific example for the input text and the output\n",
    "print(inputs[1995], outputs[1995])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to map each character to a label and create a reverse mapping to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_labels = {ch:i for i, ch in enumerate(chars)}\n",
    "labels_char = {i:ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '$': 1, '(': 2, ',': 3, '0': 4, '4': 5, '8': 6, 'D': 7, 'H': 8, 'L': 9, 'P': 10, 'T': 11, 'X': 12, '`': 13, 'd': 14, 'h': 15, 'l': 16, 'p': 17, 't': 18, 'x': 19, \"'\": 20, '+': 21, '/': 22, '3': 23, '7': 24, ';': 25, '\\xbc': 26, '?': 27, 'C': 28, 'G': 29, 'K': 30, 'O': 31, 'S': 32, 'W': 33, '[': 34, '_': 35, 'c': 36, 'g': 37, 'k': 38, 'o': 39, 's': 40, 'w': 41, '\\n': 42, '\\x95': 43, '\"': 44, '&': 45, '*': 46, '.': 47, '2': 48, '6': 49, ':': 50, '\\xbd': 51, 'B': 52, 'F': 53, 'J': 54, 'N': 55, 'R': 56, 'V': 57, 'Z': 58, 'b': 59, 'f': 60, 'j': 61, 'n': 62, 'r': 63, 'v': 64, 'z': 65, '!': 66, '\\xa2': 67, '%': 68, ')': 69, '-': 70, '1': 71, '5': 72, '9': 73, 'A': 74, '\\xc2': 75, 'E': 76, 'I': 77, 'M': 78, 'Q': 79, 'U': 80, 'Y': 81, ']': 82, 'a': 83, 'e': 84, 'i': 85, 'm': 86, 'q': 87, 'u': 88, 'y': 89}\n"
     ]
    }
   ],
   "source": [
    "print(char_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: '$', 2: '(', 3: ',', 4: '0', 5: '4', 6: '8', 7: 'D', 8: 'H', 9: 'L', 10: 'P', 11: 'T', 12: 'X', 13: '`', 14: 'd', 15: 'h', 16: 'l', 17: 'p', 18: 't', 19: 'x', 20: \"'\", 21: '+', 22: '/', 23: '3', 24: '7', 25: ';', 26: '\\xbc', 27: '?', 28: 'C', 29: 'G', 30: 'K', 31: 'O', 32: 'S', 33: 'W', 34: '[', 35: '_', 36: 'c', 37: 'g', 38: 'k', 39: 'o', 40: 's', 41: 'w', 42: '\\n', 43: '\\x95', 44: '\"', 45: '&', 46: '*', 47: '.', 48: '2', 49: '6', 50: ':', 51: '\\xbd', 52: 'B', 53: 'F', 54: 'J', 55: 'N', 56: 'R', 57: 'V', 58: 'Z', 59: 'b', 60: 'f', 61: 'j', 62: 'n', 63: 'r', 64: 'v', 65: 'z', 66: '!', 67: '\\xa2', 68: '%', 69: ')', 70: '-', 71: '1', 72: '5', 73: '9', 74: 'A', 75: '\\xc2', 76: 'E', 77: 'I', 78: 'M', 79: 'Q', 80: 'U', 81: 'Y', 82: ']', 83: 'a', 84: 'e', 85: 'i', 86: 'm', 87: 'q', 88: 'u', 89: 'y'}\n"
     ]
    }
   ],
   "source": [
    "print(labels_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start constructing our numerical input 3-tensor and output matrix. Each input example (i.e. a sequence of characters) is turned into a matrix of one-hot vectors; that is, a bunch of vectors where the index corresponding to the character is set to 1 and all the rest are set to zero.\n",
    "\n",
    "For example, if we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assuming max_len = 7\n",
    "# so our examples have 7 characters, e.g these ones:\n",
    "example = 'cab dab'\n",
    "# these are the character 'codes':\n",
    "example_char_labels = {\n",
    "    'a': 0,\n",
    "    'b': 1,\n",
    "    'c': 2,\n",
    "    'd': 3,\n",
    "    ' ' : 4\n",
    "}\n",
    "\n",
    "# matrix form\n",
    "# the example uses only five kinds of characters,\n",
    "# so the vectors only need to have five components,\n",
    "# and since the input phrase has seven characters,\n",
    "# the matrix has seven vectors.\n",
    "[\n",
    "    [0, 0, 1, 0, 0], # c\n",
    "    [1, 0, 0, 0, 0], # a\n",
    "    [0, 1, 0, 0, 0], # b\n",
    "    [0, 0, 0, 0, 1], # (space)\n",
    "    [0, 0, 0, 1, 0], # d\n",
    "    [1, 0, 0, 0, 0], # a\n",
    "    [0, 1, 0, 0, 0]  # b\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That matrix represents a _single_ training example, so for our full set of training examples, we'd have a stack of those matrices (hence a 3-tensor).\n",
    "\n",
    "![A 3-tensor of training examples](../assets/rnn_3tensor.png)\n",
    "\n",
    "And the outputs for each example are each a one-hot vector (i.e. a single character). With that in mind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980939, 20, 90)\n",
      "(980939, 90)\n"
     ]
    }
   ],
   "source": [
    "# using bool to reduce memory usage\n",
    "X = np.zeros((len(inputs), max_len, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(inputs), len(chars)), dtype=np.bool)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_labels[char]] = 1\n",
    "    y[i, char_labels[outputs[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fae024cdcd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae78a12e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let us plot a specific sentence\n",
    "plt.imshow(X[1345,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# Let us look at an example input\n",
    "print(X[13,15,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# Let us look at an example label\n",
    "print(y[230,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data, we can start training. Keras also makes this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more epochs is usually better, but training can be very slow if not on a GPU\n",
    "#epochs = 10\n",
    "#model.fit(X, y, batch_size=128, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's much more fun to see your network's ramblings as it's training, so let's write a function to produce text from the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.29640983  0.18384362  0.18384362  0.33590293]\n",
      "[0, 1, 2, 3]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Let us see how to sample from a Boltzmann distribution with parameters probs and temperature\n",
    "probs=[3,1,1,4]\n",
    "temperature=2.3\n",
    "\n",
    "a = np.log(probs)/temperature\n",
    "dist = np.exp(a)/np.sum(np.exp(a))\n",
    "choices = range(len(probs))\n",
    "print(dist)\n",
    "print(choices)\n",
    "print(np.random.choice(choices, p=dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to draw samples from a Boltzmann distribution\n",
    "def sample(probs, temperature):\n",
    "    \"\"\"samples an index from a vector of probabilities\n",
    "    (this is not the most efficient way but is more robust)\"\"\"\n",
    "    a = np.log(probs)/temperature\n",
    "    dist = np.exp(a)/np.sum(np.exp(a))\n",
    "    choices = range(len(probs))\n",
    "    return np.random.choice(choices, p=dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature=0.35 \n",
    "seed=None \n",
    "num_chars=100\n",
    "\n",
    "#Let us define a lambda function that checks if a given sentence is a long enough\n",
    "predict_more=lambda x: len(x) < num_chars\n",
    "predict_more('This sentence is too short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "ested on June 6 last\n"
     ]
    }
   ],
   "source": [
    "print(max_len)\n",
    "# Let us select a random seed sentences\n",
    "start_idx = random.randint(0, len(text) - max_len - 1)\n",
    "seed = text[start_idx:start_idx + max_len]\n",
    "\n",
    "sentence = seed\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 90)\n"
     ]
    }
   ],
   "source": [
    "# generate the input tensor\n",
    "# from the last max_len characters generated so far\n",
    "x = np.zeros((1, max_len, len(chars)))\n",
    "for t, char in enumerate(sentence):\n",
    "    x[0, t, char_labels[char]] = 1.\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fae02404650>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae0242add0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 1s 743ms/step\n"
     ]
    }
   ],
   "source": [
    "probs = model.predict(x, verbose=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01115727  0.01116542  0.01109703  0.0111792   0.01123523  0.01117972\n",
      "  0.01113946  0.01123195  0.0111145   0.01115189  0.01119096  0.01123465\n",
      "  0.01100202  0.01115845  0.01096943  0.0110179   0.0112041   0.01111914\n",
      "  0.01109436  0.01118974  0.0111216   0.01111965  0.01111599  0.01118744\n",
      "  0.01110291  0.01112745  0.01111657  0.0110405   0.01106702  0.0111851\n",
      "  0.01117395  0.01106587  0.01108769  0.01104205  0.01101129  0.01098775\n",
      "  0.01115586  0.01113332  0.01102978  0.01102076  0.01106748  0.01093825\n",
      "  0.01114987  0.01118618  0.01122334  0.01110063  0.01111811  0.01095905\n",
      "  0.01112167  0.01115206  0.01105241  0.01116264  0.01111232  0.01116625\n",
      "  0.01112509  0.01102434  0.01096045  0.01101902  0.01120701  0.01093826\n",
      "  0.01119568  0.01127507  0.01111641  0.01109456  0.01107735  0.01112289\n",
      "  0.01101889  0.01112349  0.01119363  0.01102263  0.01115104  0.01104581\n",
      "  0.01113922  0.01108365  0.0111224   0.01102477  0.01098838  0.01122756\n",
      "  0.01104307  0.01099831  0.01106829  0.01107594  0.01119472  0.01119787\n",
      "  0.01108906  0.01120105  0.0111666   0.01103473  0.01118703  0.01121053]\n",
      "(90,)\n"
     ]
    }
   ],
   "source": [
    "print(probs)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on these ideas, let us create a generate function\n",
    "def generate(temperature=0.35, seed=None, num_chars=100):\n",
    "    predict_more=lambda x: len(x) < num_chars\n",
    "    \n",
    "    if seed is not None and len(seed) < max_len:\n",
    "        raise Exception('Seed text must be at least {} chars long'.format(max_len))\n",
    "\n",
    "    # if no seed text is specified, randomly select a chunk of text\n",
    "    else:\n",
    "        start_idx = random.randint(0, len(text) - max_len - 1)\n",
    "        seed = text[start_idx:start_idx + max_len]\n",
    "\n",
    "    sentence = seed\n",
    "    generated = sentence\n",
    "\n",
    "    while predict_more(generated):\n",
    "        # generate the input tensor\n",
    "        # from the last max_len characters generated so far\n",
    "        x = np.zeros((1, max_len, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_labels[char]] = 1.\n",
    "\n",
    "        # this produces a probability distribution over characters\n",
    "        probs = model.predict(x, verbose=0)[0]\n",
    "\n",
    "        # sample the character to use based on the predicted probabilities\n",
    "        next_idx = sample(probs, temperature)\n",
    "        next_char = labels_char[next_idx]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _temperature_ controls how random we want the network to be. Lower temperatures favors more likely values, whereas higher temperatures introduce more and more randomness. At a high enough temperature, values will be chosen at random.\n",
    "\n",
    "With this generation function we can modify how we train the network so that we see some output at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Epoch 1/1\n",
      "144256/980939 [===>..........................] - ETA: 11:42 - loss: 2.3624"
     ]
    }
   ],
   "source": [
    "# Let us train for 10 epochs\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    print('epoch %d'%i)\n",
    "\n",
    "    # set nb_epoch to 1 since we're iterating manually\n",
    "    # comment this out if you just want to generate text\n",
    "    model.fit(X, y, batch_size=128, epochs=1)\n",
    "\n",
    "    # preview\n",
    "    for temp in [0.2, 0.5, 1., 1.2]:\n",
    "        print('temperature: %0.2f'%temp)\n",
    "        print('%s'%generate(temperature=temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about all there is to it. Let's try to generate one long sample passage with 2000 characters. We'll arbitrarily pick a temperature of 0.4, which seems to work decently well -- enough randomness without being incoherent. We'll also give it a seed this time (starting text): \"Today, we are facing an important challenge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rom the Middle East and the Union and to the Congress and the country is the world is to be material the parts of the Congress, the earlier than the State of the Union has been strengthened to the proposal of our community and the partnership with the production of the people of the United States of the Union and the heritage of the international and more than a meant to the world that the state of the world. \n",
      "I am confident that they are attained to strengthen the proposal of the American people was the past year our security and the responsibility of our national interest rates and all the partnership of the Congress to stand and the world the highest program of our strength and the power of the first time to the terrorists and the State and the Congress and the state of the Union is the production of our country. \n",
      "This single and manufacturing the first status of the Congress, the our first significance that they are the increased program of peace and complete standards of the same time and expected the world that this country have been a new and friends. In this country and the parts and the greatest proposal of the farmers will be a fair and respondent and the concern of the allies to achieve the very land of the world. \n",
      "I am confident that they will enact children that we can and we are not considered and the transportation of the rate of the Congress and the people of the armament of the Congress and the way to continue to work. \n",
      "The first time to do the people of course and the development of this country. \n",
      "I am confident that they will work. \n",
      "The burdens of the Union to the Congress is the first is to take the things that we can be able to provide the difference in the people of the new proposal of the industries that we have a standard of our farmers and national power of the international strength to our future and a concentration of all the strength of the first time when we have been proposed that they will continue to be proposed to make the way in the\n"
     ]
    }
   ],
   "source": [
    "print('%s' % generate(temperature=0.4, seed='Today, we are facing an important challenge.', num_chars=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
