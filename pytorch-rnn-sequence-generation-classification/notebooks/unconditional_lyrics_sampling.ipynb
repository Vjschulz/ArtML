{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "all_characters = string.printable\n",
    "number_of_characters = len(all_characters)\n",
    "\n",
    "\n",
    "def character_to_label(character):\n",
    "    \"\"\"Returns a one-hot-encoded tensor given a character.\n",
    "    \n",
    "    Uses string.printable as a dictionary.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    character : str\n",
    "        A character\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    one_hot_tensor : Tensor of shape (1, number_of_characters)\n",
    "        One-hot-encoded tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    character_label = all_characters.find(character)\n",
    "        \n",
    "    return character_label\n",
    "\n",
    "def string_to_labels(character_string):\n",
    "    \n",
    "    return map(lambda character: character_to_label(character), character_string)\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Converts labels into one-hot encoding and runs a linear\n",
    "        # layer on each of the converted one-hot encoded elements\n",
    "        \n",
    "        # input_size -- size of the dictionary + 1 (accounts for padding constant)\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequences, input_sequences_lengths, hidden=None):\n",
    "        \n",
    "        batch_size = input_sequences.shape[1]\n",
    "\n",
    "        embedded = self.encoder(input_sequences)\n",
    "\n",
    "        # Here we run rnns only on non-padded regions of the batch\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_sequences_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        \n",
    "        logits = self.logits_fc(outputs)\n",
    "        \n",
    "        logits = logits.transpose(0, 1).contiguous()\n",
    "        \n",
    "        logits_flatten = logits.view(-1, self.num_classes)\n",
    "        \n",
    "        return logits_flatten, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = RNN(input_size=len(all_characters) + 1, hidden_size=512, num_classes=len(all_characters))\n",
    "rnn.load_state_dict(torch.load('models/unconditional_lyrics_rnn.pth'))\n",
    "rnn.cuda()\n",
    "\n",
    "def sample_from_rnn(starting_sting=\"Why\", sample_length=300, temperature=1):\n",
    "\n",
    "    sampled_string = starting_sting\n",
    "    hidden = None\n",
    "\n",
    "    first_input = torch.LongTensor( string_to_labels(starting_sting) ).cuda()\n",
    "    first_input = first_input.unsqueeze(1)\n",
    "    current_input = Variable(first_input)\n",
    "\n",
    "    output, hidden = rnn(current_input, [len(sampled_string)], hidden=hidden)\n",
    "\n",
    "    output = output[-1, :].unsqueeze(0)\n",
    "\n",
    "    for i in xrange(sample_length):\n",
    "\n",
    "        output_dist = nn.functional.softmax( output.view(-1).div(temperature) ).data\n",
    "\n",
    "        predicted_label = torch.multinomial(output_dist, 1)\n",
    "\n",
    "        sampled_string += all_characters[int(predicted_label[0])]\n",
    "\n",
    "        current_input = Variable(predicted_label.unsqueeze(1))\n",
    "\n",
    "        output, hidden = rnn(current_input, [1], hidden=hidden)\n",
    "    \n",
    "    return sampled_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/repos/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The end of the day  \n",
      "I felt the love that I was lost  \n",
      "I saw the world a shadow  \n",
      "I saw the stars above  \n",
      "I saw the friend of my days  \n",
      "With my soul and the days  \n",
      "That was my destiny  \n",
      "  \n",
      "The story ends  \n",
      "The sound of the blue  \n",
      "The tears were shining  \n",
      "The story of my life  \n",
      "I still believe  \n",
      "The story of my life  \n",
      "  \n",
      "For the stars above  \n",
      "The love I feel  \n",
      "The stars in the sky  \n",
      "The stars are bright  \n",
      "The stars above  \n",
      "  \n",
      "The stars are bright  \n",
      "The stars are bright  \n",
      "The stars are bright  \n",
      "The stars\n"
     ]
    }
   ],
   "source": [
    "print(sample_from_rnn(temperature=0.5, starting_sting=\"The end\", sample_length=500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
